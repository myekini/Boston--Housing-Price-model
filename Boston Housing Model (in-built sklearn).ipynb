{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "48afa2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "587f2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"housing.csv\"\n",
    "#load data and read into dataframe\n",
    "califonia_data = pd.read_csv(PATH)\n",
    "\n",
    "#drop down NaN and display dataframe\n",
    "#califonia_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "a931a064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'median_house_value', 'ocean_proximity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "califonia_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "6d42ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting prediction target(house value)\n",
    "y = califonia_data.median_house_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "56125799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features to be considered for prediction\n",
    "features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'population', 'households', 'median_income', 'total_bedrooms', 'ocean_proximity',]\n",
    "\n",
    "#setting Features\n",
    "X = califonia_data[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "ee4622da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to get training and validation data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X,y, random_state=0)\n",
    "\n",
    "#train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "74b79aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for prediting and evaluating our dataset\n",
    "def score_all(train_X, val_X, train_y, val_y):\n",
    "    model = RandomForestRegressor(random_state=1)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y,preds)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "d1d9afba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical variables\n",
      "['ocean_proximity']\n"
     ]
    }
   ],
   "source": [
    "# columns with categorical variables \n",
    "s = (train_X.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"categorical variables\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "71f60daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first approach is to drop thes columns \n",
    "train_X_dropped = train_X.select_dtypes(exclude=['object'])\n",
    "val_X_dropped = val_X.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "bad192f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second approach is to ordinal encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "#make copy to avoid changing original data\n",
    "label_X_train = train_X.copy()\n",
    "label_X_val = val_X.copy()\n",
    "\n",
    "#apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(train_X[object_cols])\n",
    "label_X_val[object_cols] = ordinal_encoder.fit_transform(val_X[object_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "e84a2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd approach for handling categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown= 'ignore', sparse= False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_X[object_cols]))\n",
    "OH_cols_val = pd.DataFrame(OH_encoder.fit_transform(val_X[object_cols]))\n",
    "\n",
    "#one hot encoding removed index. put it back\n",
    "OH_cols_train.index = train_X.index\n",
    "OH_cols_val.index = val_X.index\n",
    "\n",
    "#removed categorical columns (will replace with one-hot encoding)\n",
    "num_train_X = train_X.drop(object_cols, axis=1)\n",
    "num_val_X = val_X.drop(object_cols, axis=1)\n",
    "\n",
    "# add one - hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_train_X, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_val_X, OH_cols_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "12948181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from dropping columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32195.37646511628"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First (1st) approach to handling missing data, get columns with missing data\n",
    "col_with_missing = [col for col in label_X_train.columns\n",
    "                   if label_X_train[col].isnull().any()]\n",
    "\n",
    "train_X_reduced = label_X_train.drop(col_with_missing, axis=1)\n",
    "val_X_reduced = label_X_val.drop(col_with_missing, axis=1)\n",
    "\n",
    "print (\"MAE from dropping columns\")\n",
    "score_all(train_X_reduced, val_X_reduced, train_y, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "c0eba0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from 2nd approach \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32406.613267441862"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second approach to handling missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Fill in the lines below: imputation\n",
    "my_imputer = SimpleImputer() # Your code here\n",
    "imputed_train_X = pd.DataFrame(my_imputer.fit_transform(label_X_train))\n",
    "imputed_val_X = pd.DataFrame(my_imputer.transform(label_X_val))\n",
    "\n",
    "# Fill in the lines below: imputation removed column names; put them back\n",
    "imputed_train_X.columns = label_X_train.columns\n",
    "imputed_val_X.columns = label_X_val.columns\n",
    "\n",
    "print(\"MAE from 2nd approach \")\n",
    "score_all(imputed_train_X, imputed_val_X, train_y, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "b23687e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammed\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\muhammed\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\muhammed\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\muhammed\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\muhammed\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from 3rd approach \n",
      "31781.240424418604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammed\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#thrid (3rd) approach to hadling missing data\n",
    "\n",
    "# making copy of the data to avoid changing originall data\n",
    "train_X_plus = OH_X_train.copy()\n",
    "val_X_plus  = OH_X_valid.copy()\n",
    "\n",
    "#find columns with missing data\n",
    "col_with_missing = [col for col in OH_X_train.columns\n",
    "                   if OH_X_train[col].isnull().any()]\n",
    "\n",
    "#looping through the missing columns to add extra information\n",
    "for col in col_with_missing:\n",
    "    train_X_plus[col + '__was missing'] = train_X_plus[col].isnull()\n",
    "    val_X_plus[col + '__was missing'] = val_X_plus[col].isnull()\n",
    "    \n",
    "#imputer\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_train_X_plus = pd.DataFrame(my_imputer.fit_transform(train_X_plus))\n",
    "imputed_val_X_plus = pd.DataFrame(my_imputer.fit_transform(val_X_plus))\n",
    "\n",
    "#fix column names\n",
    "imputed_train_X_plus.columns = train_X_plus.columns\n",
    "imputed_val_X_plus.columns = val_X_plus.columns\n",
    "\n",
    "print(\"MAE from 3rd approach \")\n",
    "print(score_all(imputed_train_X_plus, imputed_val_X_plus, train_y, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "862649de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42881.26162790698"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the model with a random state equals 1\n",
    "califonia_model = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "#fit data and #make predictions\n",
    "califonia_model.fit(imputed_train_X, train_y)\n",
    "preds = califonia_model.predict(imputed_val_X)\n",
    "\n",
    "def scoreall(val_y,preds):\n",
    "    mae = mean_absolute_error(val_y,preds)\n",
    "    return mae\n",
    "\n",
    "scoreall(val_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "74e682ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32406.613267441862"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a better predictions with RandomForestRegressor and make predictions\n",
    "\n",
    "califonia_model_2 = RandomForestRegressor(random_state=1)\n",
    "califonia_model_2.fit(imputed_train_X, train_y)\n",
    "preds_2 = califonia_model_2.predict(imputed_val_X)\n",
    "\n",
    "\n",
    "#measuring the quality of the data\n",
    "\n",
    "def scoreall(val_y,preds_2 ):\n",
    "    mae = mean_absolute_error(val_y,preds_2)\n",
    "    return mae\n",
    "\n",
    "scoreall(val_y, preds_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
